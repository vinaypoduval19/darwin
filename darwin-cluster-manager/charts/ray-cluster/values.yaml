# Default values for ray-cluster.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

# The KubeRay community welcomes PRs to expose additional configuration
# in this Helm chart.

image:
  repository: localhost:5000/ray
  tag: 2.37.0
  pullPolicy: Always

nameOverride: "kuberay"
fullnameOverride: ""

imagePullSecrets: [ ]
# - name: an-existing-secret

serviceAccount:
  create: false
  name: darwin-ds-role

# common defined values shared between the head and worker
common:
  # containerEnv specifies environment variables for the Ray head and worker containers.
  # Follows standard K8s container env schema.
  containerEnv:
    - name: SPARK_CONF_DIR
      value: /tmp/script
    - name: TERMINATE_AFTER
      value: "60"
    - name: CLUSTER_NAME
      value: id-test
    - name: CLUSTER_ID
      value: id-test
    - name: INIT_SCRIPT_API
      value: darwin-compute-uat.dream11.local/init-script-status
    - name: RAY_PROMETHEUS_HOST
      value: http://id-test-prometheus.prometheus.svc.cluster.local:9090
    - name: RAY_GRAFANA_HOST
      value: http://id-test-grafana.prometheus.svc.cluster.local:3000
    - name: RAY_GRAFANA_IFRAME_HOST
      value: https://localhost/eks-0/id-test-metrics
    - name: ENV
      value: id-test
    - name: CREATED_BY
      value: darwin@dream11.com
    - name: CLOUD
      value: AWS
    - name: RSS
      value: "False"

livenessProbe:
  # Liveness Probe settings for all the pods of the Ray Cluster.
  failureThreshold: 10
  initialDelaySeconds: 60
  periodSeconds: 15
  successThreshold: 1
  timeoutSeconds: 3

readinessProbe:
  # Readiness Probe settings for all the pods of the Ray Cluster.
  failureThreshold: 2
  initialDelaySeconds: 60
  periodSeconds: 15
  successThreshold: 1
  timeoutSeconds: 3

head:
  # rayVersion determines the autoscaler's image version.
  # It should match the Ray version in the image of the containers.
  # rayVersion: 2.9.0
  # If enableInTreeAutoscaling is true, the autoscaler sidecar will be added to the Ray head pod.
  # Ray autoscaler integration is supported only for Ray versions >= 1.11.0
  # Ray autoscaler integration is Beta with KubeRay >= 0.3.0 and Ray >= 2.0.0.
  enableInTreeAutoscaling: true
    # autoscalerOptions is an OPTIONAL field specifying configuration overrides for the Ray autoscaler.
    # The example configuration shown below represents the DEFAULT values.
    # autoscalerOptions:
    # upscalingMode: Default
    # idleTimeoutSeconds is the number of seconds to wait before scaling down a worker pod which is not using Ray resources.
    # idleTimeoutSeconds: 60
    # imagePullPolicy optionally overrides the autoscaler container's default image pull policy (IfNotPresent).
    # imagePullPolicy: IfNotPresent
    # Optionally specify the autoscaler container's securityContext.
    # securityContext: {}
    # env: []
    # envFrom: []
    # resources specifies optional resource request and limit overrides for the autoscaler container.
    # For large Ray clusters, we recommend monitoring container resource usage to determine if overriding the defaults is required.
    # resources:
    #   limits:
    #     cpu: "500m"
    #     memory: "512Mi"
    #   requests:
    #     cpu: "500m"
  #     memory: "512Mi"
  labels: { }
  # Note: From KubeRay v0.6.0, users need to create the ServiceAccount by themselves if they specify the `serviceAccountName`
  # in the headGroupSpec. See https://github.com/ray-project/kuberay/pull/1128 for more details.
  serviceAccountName: "darwin-ds-role"
  rayStartParams:
    port: "6379"
    dashboard-host: "0.0.0.0"
    num-cpus: "0"  # can be auto-completed from the limits
    node-ip-address: $MY_POD_IP  # auto-completed as the head pod IP
    block: "true"
    metrics-export-port: "8080"
    redis-password: ""
  # containerEnv specifies environment variables for the Ray container,
  # Follows standard K8s container env schema.
  containerEnv:
    - name: TYPE
      value: head
    - name: SHELL
      value: /bin/bash
    - name: MY_POD_IP
      valueFrom:
        fieldRef:
          fieldPath: status.podIP
  # - name: EXAMPLE_ENV
  #   value: "1"
  envFrom:
    - secretRef:
        name: secret-manager-token
        optional: true
  # - secretRef:
  #     name: my-env-secret
  # ports optionally allows specifying ports for the Ray container.
  ports:
    - containerPort: 6379
      name: gcs
    - containerPort: 8265
      name: dashboard
    - containerPort: 10001
      name: client
    - containerPort: 8000
      name: serve
    - containerPort: 8888
      name: jupyter
    - containerPort: 8080
      name: metrics
    - containerPort: 4040
      name: sparkui
    - containerPort: 3000
      name: vscode
    - containerPort: 8998
      name: livy
    - containerPort: 10000
      name: thrift
    - containerPort: 8001
      name: custom-1
    - containerPort: 8002
      name: custom-2
    - containerPort: 8003
      name: custom-3
  # resource requests and limits for the Ray head container.
  # Modify as needed for your application.
  # Note that the resources in this example are much too small for production;
  # we don't recommend allocating less than 8G memory for a Ray pod in production.
  # Ray pods should be sized to take up entire K8s nodes when possible.
  # Always set CPU and memory limits for Ray pods.
  # It is usually best to set requests equal to limits.
  # See https://docs.ray.io/en/latest/cluster/kubernetes/user-guides/config.html#resources
  # for further guidance.
  resources:
    limits:
      cpu: "2"
      # To avoid out-of-memory issues, never allocate less than 2G memory for the Ray head.
      memory: "4G"
    requests:
      cpu: "2"
      memory: "4G"
  annotations:
    karpenter.sh/do-not-disrupt: "true"
  nodeSelector: {}
  tolerations: [ ]
  affinity: { }
  # Ray container security context.
  securityContext:
    runAsGroup: 100
    runAsUser: 1000
    capabilities:
      add:
        - SYS_PTRACE
  # Optional: The following volumes/volumeMounts configurations are optional but recommended because
  # Ray writes logs to /tmp/ray/session_latests/logs instead of stdout/stderr.
  volumes:
    - name: log-volume
      emptyDir: { }
    - name: id-test-scripts-vol
      configMap:
        name: id-test-script
        defaultMode: 511
    - name: id-test-remote-command-vol
      configMap:
        name: id-test-remote-command
        defaultMode: 0777
    - name: persistent-storage
      persistentVolumeClaim:
        claimName: fsx-claim-3
  volumeMounts:
    - mountPath: "/tmp/ray"
      name: log-volume
    - mountPath: "/tmp/script"
      name: id-test-scripts-vol
    - mountPath: "/tmp/remote-command"
      name: id-test-remote-command-vol
    - name: persistent-storage
      mountPath: "/home/ray/fsx"
  # sidecarContainers specifies additional containers to attach to the Ray pod.
  # Follows standard K8s container spec.
  sidecarContainers: [ ]
  # See docs/guidance/pod-command.md for more details about how to specify
  # container command for head Pod.
  command: [ ]
  args:
    - "/tmp/script/head.sh"
  # Optional, for the user to provide any additional fields to the service.
  # See https://pkg.go.dev/k8s.io/Kubernetes/pkg/api/v1#Service
  headService: { }
  # metadata:
  #   annotations:
  #     prometheus.io/scrape: "true"


worker:
  # If you want to disable the default workergroup
  # uncomment the line below
  # disabled: true
  groupName: wg
  replicas: 1
  minReplicas: 1
  maxReplicas: 3
  labels: { }
  serviceAccountName: "darwin-ds-role"
  rayStartParams:
    node-ip-address: $MY_POD_IP
    redis-password: LetMeInRay
    block: "true"
    metrics-export-port: "8080"
    resources: '"{\"ondemand\": 100}"'
  # containerEnv specifies environment variables for the Ray container,
  # Follows standard K8s container env schema.
  containerEnv:
    - name: TYPE
      value: worker
    - name: RAY_DISABLE_DOCKER_CPU_WARNING
      value: "1"
    - name: MY_POD_NAME
      valueFrom:
        fieldRef:
          fieldPath: metadata.name
    - name: MY_POD_IP
      valueFrom:
        fieldRef:
          fieldPath: status.podIP
    - name: CPU_REQUEST
      valueFrom:
        resourceFieldRef:
          containerName: ray-worker
          resource: requests.cpu
  envFrom:
    - secretRef:
        name: secret-manager-token
        optional: true
  # - secretRef:
  #     name: my-env-secret
  # ports optionally allows specifying ports for the Ray container.
  ports:
    - containerPort: 8080
      name: metrics
  # resource requests and limits for the Ray head container.
  # Modify as needed for your application.
  # Note that the resources in this example are much too small for production;
  # we don't recommend allocating less than 8G memory for a Ray pod in production.
  # Ray pods should be sized to take up entire K8s nodes when possible.
  # Always set CPU and memory limits for Ray pods.
  # It is usually best to set requests equal to limits.
  # See https://docs.ray.io/en/latest/cluster/kubernetes/user-guides/config.html#resources
  # for further guidance.
  resources:
    limits:
      cpu: "2"
      memory: "4G"
    requests:
      cpu: "2"
      memory: "4G"
  annotations:
    karpenter.sh/do-not-disrupt: "true"
  nodeSelector: {}
  tolerations: [ ]
  affinity: { }
  # Ray container security context.
  securityContext:
    capabilities:
      add:
        - SYS_PTRACE
  # Optional: The following volumes/volumeMounts configurations are optional but recommended because
  # Ray writes logs to /tmp/ray/session_latests/logs instead of stdout/stderr.
  volumes:
    - name: log-volume
      emptyDir: { }
    - name: id-test-scripts-vol
      configMap:
        name: id-test-script
        defaultMode: 0777
    - name: id-test-remote-command-vol
      configMap:
        name: id-test-remote-command
        defaultMode: 0777
  volumeMounts:
    - mountPath: /tmp/ray
      name: log-volume
    - mountPath: /tmp/script
      name: id-test-scripts-vol
    - mountPath: /tmp/remote-command
      name: id-test-remote-command-vol
  # sidecarContainers specifies additional containers to attach to the Ray pod.
  # Follows standard K8s container spec.
  sidecarContainers: [ ]
  # See docs/guidance/pod-command.md for more details about how to specify
  # container command for worker Pod.
  command: [ ]
  args:
    - "/tmp/script/worker.sh"

# The map's key is used as the groupName.
# For example, key:small-group in the map below
# will be used as the groupName
additionalWorkerGroups:
  smallGroup:
    # Disabled by default
    disabled: true
    replicas: 0
    minReplicas: 0
    maxReplicas: 3
    labels: { }
    serviceAccountName: "darwin-ds-role"
    rayStartParams:
      node-ip-address: $MY_POD_IP
      redis-password: LetMeInRay
      block: "true"
      metrics-export-port: "8080"
      resources: '"{\"spot\": 200}"'
    # containerEnv specifies environment variables for the Ray container,
    # Follows standard K8s container env schema.
    containerEnv:
      - name: TYPE
        value: worker
      - name: RAY_DISABLE_DOCKER_CPU_WARNING
        value: "1"
      - name: MY_POD_IP
        valueFrom:
          fieldRef:
            fieldPath: status.podIP
      - name: MY_POD_NAME
        valueFrom:
          fieldRef:
            fieldPath: metadata.name
      - name: CPU_REQUEST
        valueFrom:
          resourceFieldRef:
            containerName: ray-worker
            resource: requests.cpu
    envFrom:
      - secretRef:
          name: secret-manager-token
          optional: true
    # - secretRef:
    #     name: my-env-secret
    # ports optionally allows specifying ports for the Ray container.
    ports:
      - containerPort: 8080
        name: metrics
    # resource requests and limits for the Ray head container.
    # Modify as needed for your application.
    # Note that the resources in this example are much too small for production;
    # we don't recommend allocating less than 8G memory for a Ray pod in production.
    # Ray pods should be sized to take up entire K8s nodes when possible.
    # Always set CPU and memory limits for Ray pods.
    # It is usually best to set requests equal to limits.
    # See https://docs.ray.io/en/latest/cluster/kubernetes/user-guides/config.html#resources
    # for further guidance.
    resources:
      limits:
        cpu: 1
        memory: "1G"
      requests:
        cpu: 1
        memory: "1G"
    annotations:
      karpenter.sh/do-not-disrupt: "true"
    nodeSelector: {}
    tolerations: [ ]
    affinity: {}
    # Ray container security context.
    securityContext:
      capabilities:
        add:
          - SYS_PTRACE
    # Optional: The following volumes/volumeMounts configurations are optional but recommended because
    # Ray writes logs to /tmp/ray/session_latests/logs instead of stdout/stderr.
    volumes:
      - name: log-volume
        emptyDir: { }
      - name: id-test-scripts-vol
        configMap:
          name: id-test-script
          defaultMode: 511
      - name: id-test-remote-command-vol
        configMap:
          name: id-test-remote-command
          defaultMode: 0777
    volumeMounts:
      - mountPath: /tmp/ray
        name: log-volume
      - mountPath: /tmp/script
        name: id-test-scripts-vol
      - mountPath: /tmp/remote-command
        name: id-test-remote-command-vol
    sidecarContainers: [ ]
    # See docs/guidance/pod-command.md for more details about how to specify
    # container command for worker Pod.
    command: [ ]
    args:
      - "/tmp/script/worker.sh"

# Configuration for Head's Kubernetes Service
service:
  # This is optional, and the default is ClusterIP.
  type: ClusterIP

grafana:
  image: "localhost:5000/grafana:latest"
  rootPath: "/eks-0/id-test-metrics/"
  nodeSelector: {}

prometheus:
  replicas: 1
  nodeSelector: {}
  remoteWrite:
    - url: "http://darwin-thanos:10908/api/v1/receive"

commands: [ ]
sparkConfig:
  spark.ui.proxyBase: "/eks-0/id-test-sparkui"
  spark.ui.proxyRedirectUri: "/"
  spark.metrics.conf.*.sink.prometheusServlet.class: org.apache.spark.metrics.sink.PrometheusServlet
  spark.metrics.conf.*.sink.prometheusServlet.path: "/metrics/prometheus"
  spark.metrics.conf.master.sink.prometheusServlet.path: "/metrics/master/prometheus"
  spark.metrics.conf.applications.sink.prometheusServlet.path: "/metrics/applications/prometheus"
  spark.ui.prometheus.enabled: "true"
is_job_cluster: false
env: uat
user: darwin
email: darwin@darwin.com
cluster_name: id-test
kube_cluster_key: eks-0
domain: localhost
terminate_after_minutes: 60
cluster_id: id-test
TEAM_SUFFIX: -uat
VPC_SUFFIX:
cluster_type: all_purpose_cluster

labels:
  project: darwin
  service: darwin
  squad: data-science
  environment: uat
  custom-1: custom1

remoteCommand:
  statusReportApi: "http://darwin-compute:8000/cluster/command/pod/status"
  statusReportInterval: 10
  logsS3Bucket: "darwin"
  logsS3Key: "mlp/logs/remote-command"
  commands:
    head: []
    worker: []
